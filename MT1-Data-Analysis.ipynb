{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5875fc27-f741-453e-ac90-5164d23497e6",
   "metadata": {},
   "source": [
    "## Data Analysis for Masters Thesis 1\n",
    "\n",
    "### Title: Stemming algorithms for English\n",
    "\n",
    "### Algorithms being studied\n",
    "\n",
    "- Porter\n",
    "- Lancaster\n",
    "- Snowball (Porter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155dd856-5476-41d4-b395-162cd54ac05b",
   "metadata": {},
   "source": [
    "#### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a67fe0f-384b-4a1b-93ba-1108ce97c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, snowball\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdd0a2-249b-400c-bd4e-633ba290fdca",
   "metadata": {},
   "source": [
    "#### Extracting words from the corpora and preparing them for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf3517d-7506-4c8d-baba-e6756493c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = brown.words()\n",
    "corpus = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3263e9-3d47-45eb-add3-ddb516d191c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the corpus:  1161192\n",
      "Total number of unique tokens:  56057\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in the corpus: \", len(words))\n",
    "print(\"Total number of unique tokens: \", len(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e973f2-04e9-4b4d-9de6-5f1ee570f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.lower()\n",
    "corpus = corpus.replace(\"\\n\",\" \")\n",
    "corpus = corpus.replace(\"-\", \" \")\n",
    "cleaned_corpus = [i for i in corpus if i.isalpha() or i==' ']\n",
    "cleaned_corpus = ''.join(cleaned_corpus)\n",
    "rev_corpus = [i[::-1] for i in cleaned_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df455c4d-ce29-47ad-a196-8a72b91eba8d",
   "metadata": {},
   "source": [
    "#### **Removing functional words and sorting according to the reverse of the spelling so as it order it according to the word endings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9141d18-585a-4ca3-973d-d43759eb99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_words = [\n",
    "    \"the\", \"a\", \"an\",\n",
    "    \"in\", \"on\", \"at\", \"by\", \"for\", \"with\", \"to\", \"from\", \"of\", \"about\", \"through\", \"between\", \"among\", \"under\", \"over\",\n",
    "    \"and\", \"but\", \"or\", \"nor\", \"for\", \"so\", \"yet\",\n",
    "    \"although\", \"because\", \"if\", \"unless\", \"since\", \"while\", \"when\", \"after\", \"before\", \"as\", \"though\",\n",
    "    \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"you\", \"him\", \"her\", \"us\", \"them\",\n",
    "    \"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"their\", \"mine\", \"yours\", \"hers\", \"ours\", \"theirs\",\n",
    "    \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\",\n",
    "    \"who\", \"whom\", \"whose\", \"which\", \"that\",\n",
    "    \"this\", \"that\", \"these\", \"those\",\n",
    "    \"who\", \"whom\", \"whose\", \"which\", \"what\",\n",
    "    \"all\", \"another\", \"any\", \"anybody\", \"anyone\", \"anything\", \"both\", \"each\", \"either\", \"everybody\", \"everyone\", \"everything\", \"neither\", \"nobody\", \"no one\", \"nothing\", \"several\", \"some\", \"somebody\", \"someone\", \"something\",\n",
    "    \"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\",\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"being\", \"been\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\",\n",
    "    \"also\", \"not\", \"never\", \"always\", \"very\", \"too\", \"so\", \"such\", \"here\", \"there\", \"now\", \"then\", \"when\", \"where\",\n",
    "    \"today\", \"yesterday\", \"tomorrow\", \"soon\", \"now\", \"then\", \"already\", \"lately\",\n",
    "    \"always\", \"usually\", \"often\", \"sometimes\", \"seldom\", \"never\"\n",
    "]\n",
    "\n",
    "words = cleaned_corpus.split()\n",
    "\n",
    "primary_dataset = [i for i in words if not i in functional_words]\n",
    "primary_dataset = list(set(primary_dataset))\n",
    "primary_dataset = [i[::-1] for i in primary_dataset]\n",
    "primary_dataset.sort()\n",
    "primary_dataset = [i[::-1] for i in primary_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bed057f-23aa-4405-8599-ba9b468151cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the primary dataset:  42551\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in the primary dataset: \", len(primary_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7899889-b9b3-422b-89e2-a2bc840e8597",
   "metadata": {},
   "source": [
    "### Applying systematic sampling (selecting every 10th word) total of 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35456b9-54b3-4960-8141-a2285a4f4824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size:  4256\n",
      "Sample preview:  ['aa', 'elba', 'tuba', 'jamaica', 'veronica', 'atlantica', 'dellarca', 'ywca', 'hedda', 'salida', 'veranda', 'tenda', 'soda', 'medea', 'anthea', 'andrea', 'hoffa', 'bottega', 'ticonderoga', 'mischa', 'pasha', 'bertha', 'suburbia', 'acadia', 'pharmacopoeia', 'bahia', 'malia', 'anglia', 'julia', 'lumia']\n"
     ]
    }
   ],
   "source": [
    "sample_systematic = []\n",
    "\n",
    "for i in range(0, len(primary_dataset), 10):\n",
    "    sample_systematic.append(primary_dataset[i])\n",
    "\n",
    "print(\"Sample size: \", len(sample_systematic))\n",
    "print(\"Sample preview: \", sample_systematic[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06df0e7-2e63-4f8e-973d-b2944d1fdae2",
   "metadata": {},
   "source": [
    "### Creating objects for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d74024c-5811-427d-b75d-dc590e47d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9ca51-191f-4d51-a2cc-539f9a3d1cb6",
   "metadata": {},
   "source": [
    "### Applying stemming over the primary dataset and the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eee1bc4-6d47-44db-a539-88cdc9194ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemAll(dataset):\n",
    "    stemmed_lancaster = [stemmer_lancaster.stem(i) for i in dataset]\n",
    "    stemmed_porter = [stemmer_porter.stem(i) for i in dataset]\n",
    "    stemmed_snowball = [stemmer_snowball.stem(i) for i in dataset]\n",
    "\n",
    "    df_processed = pd.DataFrame({\n",
    "    \"Original\" : dataset,\n",
    "    \"Porter\" : stemmed_porter,\n",
    "    \"Lancaster\" : stemmed_lancaster,\n",
    "    \"Snowball\" : stemmed_snowball,\n",
    "    }, index=range(1,len(dataset)+1))\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd71489a-1f0b-4ca0-be1e-52d0e172fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original   Porter Lancaster Snowball\n",
      "1       aa       aa        aa       aa\n",
      "2      aaa      aaa        aa      aaa\n",
      "3       ba       ba        ba       ba\n",
      "4  barnaba  barnaba    barnab  barnaba\n",
      "5     paba     paba       pab     paba\n",
      "\n",
      "   Original    Porter Lancaster  Snowball\n",
      "1        aa        aa        aa        aa\n",
      "2      elba      elba       elb      elba\n",
      "3      tuba      tuba       tub      tuba\n",
      "4   jamaica   jamaica    jamaic   jamaica\n",
      "5  veronica  veronica   veronic  veronica\n"
     ]
    }
   ],
   "source": [
    "df_primary = stemAll(primary_dataset)\n",
    "df_sample_systematic = stemAll(sample_systematic)\n",
    "print(df_primary.head())\n",
    "print()\n",
    "print(df_sample_systematic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4dd420-99e3-4c5e-8a1c-228fa9f78881",
   "metadata": {},
   "source": [
    "#### Average lenght of Original words and stems produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50cadd68-a729-4ebf-b100-dd32ab16aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 7.754858875232075\n",
      "Porter: 6.345867312166576\n",
      "Lancaster: 5.53432351766116\n",
      "Snowball: 6.321825574017062\n"
     ]
    }
   ],
   "source": [
    "len_original = len_porter = len_lancaster = len_snowball = 0\n",
    "\n",
    "len_primary = len(df_primary)\n",
    "\n",
    "for i in range(1, len_primary+1):\n",
    "    len_original += len(df_primary['Original'][i])\n",
    "    len_porter += len(df_primary['Porter'][i])\n",
    "    len_lancaster += len(df_primary['Lancaster'][i])\n",
    "    len_snowball += len(df_primary['Snowball'][i])\n",
    "\n",
    "print(f\"Original: {len_original/len_primary}\\nPorter: {len_porter/len_primary}\\nLancaster: {len_lancaster/len_primary}\\nSnowball: {len_snowball/len_primary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388266f-bc41-48cd-9d7d-39d04974ed13",
   "metadata": {},
   "source": [
    "### Grouping outputs on the basis of stem produced by each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f9ef0a-b80a-4346-9e1e-88743f747144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by porter\n",
    "group_porter_primary = df_primary.groupby(\"Porter\").agg(lambda x: x.tolist())\n",
    "group_porter_sample_systematic = df_sample_systematic.groupby(\"Porter\").agg(lambda x: x.tolist())\n",
    "\n",
    "# Grouping by lancaster\n",
    "group_lancaster_primary = df_primary.groupby(\"Lancaster\").agg(lambda x: x.tolist())\n",
    "group_lancaster_sample_systematic = df_sample_systematic.groupby(\"Lancaster\").agg(lambda x: x.tolist())\n",
    "\n",
    "# Grouping by snowball\n",
    "group_snowball_primary = df_primary.groupby(\"Snowball\").agg(lambda x: x.tolist())\n",
    "group_snowball_sample_systematic = df_sample_systematic.groupby(\"Snowball\").agg(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be5e86-55e9-4311-bb8e-313445a3c137",
   "metadata": {},
   "source": [
    "### Words for which all algorithms produce a common stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229b0e04-8cb3-4ca1-bf55-d477b224ca6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Original</th>\n",
       "      <th>Porter</th>\n",
       "      <th>Lancaster</th>\n",
       "      <th>Snowball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>spa</td>\n",
       "      <td>spa</td>\n",
       "      <td>spa</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>cab</td>\n",
       "      <td>cab</td>\n",
       "      <td>cab</td>\n",
       "      <td>cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>grab</td>\n",
       "      <td>grab</td>\n",
       "      <td>grab</td>\n",
       "      <td>grab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>caleb</td>\n",
       "      <td>caleb</td>\n",
       "      <td>caleb</td>\n",
       "      <td>caleb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>4252</td>\n",
       "      <td>merz</td>\n",
       "      <td>merz</td>\n",
       "      <td>merz</td>\n",
       "      <td>merz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>4253</td>\n",
       "      <td>livshitz</td>\n",
       "      <td>livshitz</td>\n",
       "      <td>livshitz</td>\n",
       "      <td>livshitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>4254</td>\n",
       "      <td>markovitz</td>\n",
       "      <td>markovitz</td>\n",
       "      <td>markovitz</td>\n",
       "      <td>markovitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>4255</td>\n",
       "      <td>schwartz</td>\n",
       "      <td>schwartz</td>\n",
       "      <td>schwartz</td>\n",
       "      <td>schwartz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>4256</td>\n",
       "      <td>fuzz</td>\n",
       "      <td>fuzz</td>\n",
       "      <td>fuzz</td>\n",
       "      <td>fuzz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2386 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   Original     Porter  Lancaster   Snowball\n",
       "0         1         aa         aa         aa         aa\n",
       "1        73        spa        spa        spa        spa\n",
       "2       104        cab        cab        cab        cab\n",
       "3       105       grab       grab       grab       grab\n",
       "4       107      caleb      caleb      caleb      caleb\n",
       "...     ...        ...        ...        ...        ...\n",
       "2381   4252       merz       merz       merz       merz\n",
       "2382   4253   livshitz   livshitz   livshitz   livshitz\n",
       "2383   4254  markovitz  markovitz  markovitz  markovitz\n",
       "2384   4255   schwartz   schwartz   schwartz   schwartz\n",
       "2385   4256       fuzz       fuzz       fuzz       fuzz\n",
       "\n",
       "[2386 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_stems_primary = df_primary[(df_primary[\"Lancaster\"] == df_primary[\"Porter\"]) & (df_primary[\"Porter\"] == df_primary[\"Snowball\"])].reset_index()\n",
    "common_stems_sample_sys = df_sample_systematic[(df_sample_systematic[\"Lancaster\"] == df_sample_systematic[\"Porter\"]) & (df_sample_systematic[\"Porter\"] == df_sample_systematic[\"Snowball\"])].reset_index()\n",
    "common_stems_sample_sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a18e4-f84c-49de-ab07-b58c27be419c",
   "metadata": {},
   "source": [
    "### Eliminating words which are not stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bad4a35-ba7a-446f-99e8-88ee90b8843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_stems_primary_stemmed = common_stems_primary[common_stems_primary[\"Original\"] != common_stems_primary[\"Porter\"]].reset_index()\n",
    "common_stems_sys_stemmed = common_stems_primary[common_stems_primary[\"Original\"] != common_stems_primary[\"Porter\"]].reset_index()\n",
    "common_stems_primary_stemmed = common_stems_primary_stemmed.drop([\"level_0\", \"index\"], axis = 1)\n",
    "common_stems_sys_stemmed = common_stems_sys_stemmed.drop([\"level_0\", \"index\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84760c46-81b6-4d67-ab9d-6b5ce138cabe",
   "metadata": {},
   "source": [
    "### Reduced size of the dataset after application of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be307010-2f93-4800-966d-6406ddf13d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 42551\n",
      "Porter: 26202\n",
      "Lancaster: 21361\n",
      "Snowball: 25797\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {len(primary_dataset)}\\nPorter: {len(group_porter_primary)}\\nLancaster: {len(group_lancaster_primary)}\\nSnowball: {len(group_snowball_primary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4166039-9af8-4dd2-acbb-9e76461ab829",
   "metadata": {},
   "source": [
    "### Where ouput from snowball varies from Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "778fbf2e-673c-4861-a6b1-b13eab3f3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_vs_snowball = df_primary[df_primary[\"Porter\"] != df_primary[\"Snowball\"]].reset_index()\n",
    "porter_vs_snowball = porter_vs_snowball.drop(\"Lancaster\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aab30-9b47-4067-aba7-08d264577f38",
   "metadata": {},
   "source": [
    "# Writing all of this to the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9596a19-8169-4c07-97bc-1ed67a81f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_primary.to_csv(\"./outputs/primary_dataset.csv\")\n",
    "df_sample_systematic.to_csv(\"./outputs/sample_dataset.csv\")\n",
    "group_porter_primary.to_csv(\"./outputs/group_porter_primary.csv\")\n",
    "group_porter_sample_systematic.to_csv(\"./outputs/group_porter_sample.csv\")\n",
    "group_lancaster_primary.to_csv(\"./outputs/group_lancaster_primary.csv\")\n",
    "group_lancaster_sample_systematic.to_csv(\"./outputs/group_lancaster_sample.csv\")\n",
    "group_snowball_primary.to_csv(\"./outputs/group_snowball_primary.csv\")\n",
    "group_snowball_sample_systematic.to_csv(\"./outputs/group_snowball_sample.csv\")\n",
    "common_stems_primary.to_csv(\"./outputs/common_stems_primary.csv\")\n",
    "common_stems_sample_sys.to_csv(\"./outputs/common_stems_sample.csv\")\n",
    "common_stems_primary_stemmed.to_csv(\"./outputs/common_stems_primary_stemmed.csv\")\n",
    "common_stems_sys_stemmed.to_csv(\"./outputs/common_stems_sample_stemmed.csv\")\n",
    "porter_vs_snowball.to_csv(\"./outputs/porter_vs_snowball.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
